{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0YjUqu2r/NxG51sYxc7e/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashsolanki162003/Workflow_of_ChatGPT_Model/blob/main/Workflow_of_ChatGPT_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcoV_7fUbQl7",
        "outputId": "8bca41b2-8446-44ba-ff21-35aba6a2e905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers sentence-transformers tiktoken faiss-cpu matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "def shortvec(v, n=8):\n",
        "  \"\"\"Return first n dimensions of a vector for compact printing.\"\"\"\n",
        "  return np.array(v).flatten()[:n].tolist()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBTkp-QGb48n",
        "outputId": "3b5aafee-0664-425e-ea27-1e7984b52291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hii\"\n",
        "print(\"User prompt:\", repr(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZOLbFjHdAT0",
        "outputId": "7e5ce5fb-7752-4d1f-d3ff-59a7566a071a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User prompt: 'Hii'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\") # encoding used by many modern GPT models\n",
        "tiktoken_ids = enc.encode(prompt)\n",
        "tiktoken_strings = [enc.decode([tid]) for tid in tiktoken_ids]\n",
        "print(\"tiktoken token IDs:\", tiktoken_ids)\n",
        "print(\"tiktoken token strings:\", tiktoken_strings)\n",
        "print(\"tiktoken token count:\", len(tiktoken_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS1laYm2dKXw",
        "outputId": "cf143de2-9a24-496d-deb9-70e00a2b3abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken token IDs: [39, 3893]\n",
            "tiktoken token strings: ['H', 'ii']\n",
            "tiktoken token count: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "gpt2_tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "# ensure pad token exists for safety in some operations\n",
        "\n",
        "gpt2_ids = gpt2_tok.encode(prompt, add_special_tokens=False)\n",
        "gpt2_tokens = gpt2_tok.convert_ids_to_tokens(gpt2_ids)\n",
        "gpt2_decoded = gpt2_tok.decode(gpt2_ids)\n",
        "\n",
        "print(\"GPT-2 token ids:\", gpt2_ids)\n",
        "print(\"GPT-2 token strings:\", gpt2_tokens)\n",
        "print(\"GPT-2 decoded:\", repr(gpt2_decoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfA-4gzMg5P7",
        "outputId": "112d4406-39d2-4719-addb-743d939a19e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 token ids: [39, 4178]\n",
            "GPT-2 token strings: ['H', 'ii']\n",
            "GPT-2 decoded: 'Hii'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # small, fast\n",
        "prompt_emb = embed_model.encode(prompt, convert_to_numpy=True)\n",
        "print(\"Prompt embedding shape:\", prompt_emb.shape)\n",
        "print(\"Prompt embedding (first dims):\", shortvec(prompt_emb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-tPzTqpiLfR",
        "outputId": "db52f9f9-64ce-4710-cd6f-8b504acbea5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt embedding shape: (384,)\n",
            "Prompt embedding (first dims): [-0.1123691201210022, 0.1150505393743515, 0.029519375413656235, 0.002057114616036415, -0.0479264035820961, -0.07721183449029922, 0.08405883610248566, -0.0073411064222455025]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embs = [embed_model.encode(t, convert_to_numpy=True) for t in tiktoken_strings]\n",
        "for i, (t, emb) in enumerate(zip(tiktoken_strings, token_embs)):\n",
        " print(f\"Token {i} '{t}' -> dim {emb.shape} -> {shortvec(emb)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9GNt0K2iviq",
        "outputId": "19e7cac6-d721-43a7-87ea-dade50f0031a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 0 'H' -> dim (384,) -> [-0.057101570069789886, 0.0941014438867569, -0.04296128824353218, 0.015190373174846172, 0.0035909090656787157, 0.019870700314641, 0.0812176913022995, 0.037185586988925934]\n",
            "Token 1 'ii' -> dim (384,) -> [-0.029413770884275436, 0.017667163163423538, -0.014803135767579079, 0.05054305121302605, -0.03761046752333641, 0.005316558293998241, 0.10382639616727829, 0.05065855011343956]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "for i, emb in enumerate(token_embs):\n",
        "  sim = float(np.dot(prompt_emb, emb) / (norm(prompt_emb) * norm(emb)))\n",
        "  print(f\"cosine(prompt, token_{i}='{tiktoken_strings[i]}') = {sim:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD8bE5lajXZl",
        "outputId": "8c3e9361-1d9d-4205-dcfd-c99af95f009b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine(prompt, token_0='H') = 0.5304\n",
            "cosine(prompt, token_1='ii') = 0.3688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", attn_implementation=\"eager\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model.config.output_hidden_states = True\n",
        "model.config.output_attentions = True\n",
        "input_ids = torch.tensor([gpt2_ids], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        " outputs = model(input_ids, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "\n",
        " logits = outputs.logits # (batch, seq_len, vocab_size)\n",
        " hidden_states = outputs.hidden_states # tuple: (embedding_output, layer1_out, ..., last_layer_out)\n",
        " attentions = outputs.attentions # tuple: one per layer (batch, num_heads, seq_len, seq_len)\n",
        "\n",
        "\n",
        " print(\"Logits shape:\", logits.shape)\n",
        " print(\"Number of hidden-state tensors (incl embeddings):\", len(hidden_states))\n",
        " print(\"Hidden state last layer shape:\", hidden_states[-1].shape)\n",
        " print(\"Number of attention tensors (layers):\", len(attentions))\n",
        " print(\"Attention shape (layer 0):\", attentions[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNGN3CJpmTgk",
        "outputId": "cff07545-b827-4329-8aa3-3ead020949c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([1, 2, 50257])\n",
            "Number of hidden-state tensors (incl embeddings): 13\n",
            "Hidden state last layer shape: torch.Size([1, 2, 768])\n",
            "Number of attention tensors (layers): 12\n",
            "Attention shape (layer 0): torch.Size([1, 12, 2, 2])\n"
          ]
        }
      ]
    }
  ]
}